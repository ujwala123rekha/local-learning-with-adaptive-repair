üß† Motivation

Backpropagation, while powerful, has several limitations:
Strong coupling between forward and backward passes
Update locking across layers
High memory and compute cost for deep networks
Limited biological plausibility
This project investigates whether competitive performance can be achieved using only forward passes and local objectives, supplemented by adaptive repair and batch-level performance comparison.

üèóÔ∏è Core Idea
The network is divided into independent segments, each trained using local signals:
Key mechanisms:

Local Heads
Each segment has its own classifier head for local cross-entropy feedback.

Next-Activation Prediction
A lightweight predictor attempts to predict the next layer‚Äôs activation, enabling representational alignment without gradients.

Adaptive Repair Mechanism

A local update is applied

Its effect is evaluated immediately

If performance degrades, parameters are reverted

If performance improves, the update is kept

Batch-wise Reward Signal

Updates are encouraged or discouraged based on global loss improvement

No gradient backpropagation across segments

EMA Stabilization

Exponential moving averages of activations reduce instability in local targets

üî¨ Training Characteristics

No global backward pass

No gradient flow across segments

Local optimization only

Forward-only information flow

Explicit parameter rollback on harmful updates

This makes the system suitable for studying:

Decoupled learning

Modular neural systems

Backpropagation alternatives

Asynchronous or biologically inspired learning

üìä Experimental Results (MNIST)
Method	Test Accuracy
Forward-only adaptive segment repair	96.36%
Standard backpropagation baseline	97.77%

Observation:
While standard backpropagation achieves higher accuracy, the forward-only method reaches competitive performance without global gradient propagation.

This gap is expected on MNIST, a task where backpropagation is highly optimized and efficient.

üß™ Interpretation

This is not intended to replace backpropagation

The goal is to study feasibility, not outperform BP

The results demonstrate that non-gradient, forward-only learning can be viable

The method trades some accuracy for modularity, interpretability, and decoupling

‚ö†Ô∏è Limitations

Computationally heavier than BP for small datasets

No formal convergence guarantees

Sensitive to hyperparameters

Currently validated only on MNIST

Designed as a research prototype, not production code
‚ö° Parallelism & Decoupling Advantages
1. Elimination of Update Locking

Standard backpropagation tightly couples all layers through the backward pass, forcing each layer to wait for gradients from downstream layers before updating. In contrast, this framework removes global backward dependency by relying only on local objectives and forward signals. As a result, each segment can update independently, enabling structurally parallel learning that is not possible with traditional backpropagation.

2. Segment-Level Asynchronous Training

Each network segment maintains its own local loss, evaluates its update immediately, and decides whether to accept or revert changes. This allows different segments to be optimized asynchronously and in parallel without requiring strict synchronization across the network. Such segment-wise autonomy naturally supports pipeline-style execution and distributed training scenarios.

3. Forward-Only Communication for Scalable Systems

By using only forward passes and avoiding gradient transmission, the method reduces memory dependency and bidirectional communication overhead. Lightweight forward signals (activations, predictions, and rewards) make the approach compatible with model parallelism, decentralized systems, and potentially neuromorphic or agent-based architectures, where synchronized backpropagation becomes a bottleneck.

üîÆ Future Work

Formal convergence analysis

Experiments on deeper networks and larger datasets

Ablation studies on repair and reward mechanisms

Hybrid methods combining partial gradients with local repair

Applications to asynchronous or agent-based systems

Energy-efficient or neuromorphic settings

üßë‚Äçüíª Authorship & Credit

Core learning idea & experiment design: G. Ujwala Rekha

Implementation: Experimental validation of the proposed concept

This repository reflects an idea-driven exploration, where implementation serves to test and observe theoretical intuition.

üìÑ Related Paper

Breaking the Chain: Decoupled Learning Through Collective Layer Feedback
G. Ujwala Rekha
Jain Deemed-to-be University
(Conceptual foundation for this implementation) 

Breaking the backpropagation

üìú License

This project is released under the MIT License.
